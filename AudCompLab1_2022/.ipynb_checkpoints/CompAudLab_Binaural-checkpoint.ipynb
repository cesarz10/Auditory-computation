{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tl_model'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-81b9f668f8f1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwavfile\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msio_wav\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignal\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msp_sig\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtl_model\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtl_cochlea\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mfilters\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0merb_point\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0merb_space\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcentre_freqs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmake_erb_filters\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0merb_filterbank\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mgammatone_analysis\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpow_stft\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mhz2mel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmel2hz\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmel_fb\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmel_analysis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tl_model'"
     ]
    }
   ],
   "source": [
    "#Main code framework to run a basic crosscorrelation-based auditory model.\n",
    "#Sarah Verhulst, Arthur Van Den Broucke, Deepak Baby, UGent, 2021\n",
    "\n",
    "import numpy as np\n",
    "import scipy.io.wavfile as sio_wav\n",
    "import scipy.signal as sp_sig\n",
    "# from tl_model import tl_cochlea\n",
    "from filters import (erb_point,erb_space,centre_freqs,make_erb_filters,erb_filterbank,gammatone_analysis,pow_stft,hz2mel,mel2hz,mel_fb,mel_analysis)\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.fftpack import dct\n",
    "from signal_utils import *\n",
    "%matplotlib inline\n",
    "import sounddevice as sd\n",
    "import soundfile as sf\n",
    "\n",
    "#Parameters for the GT filterbank\n",
    "fmin = 50. #lowest frequency simulated for GT\n",
    "numbands = 64 #number of GT bands\n",
    "\n",
    "HRTF , fsHRTF =sf.read('IRC_1023_C_R0195_T030_P000.wav')\n",
    "fs, x = sio_wav.read('example.wav')\n",
    "x=x/max(abs(x));\n",
    "\n",
    "sd.play(x, fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use np.convolve to filter the monaural x signal to \n",
    "#call the output sig, which should have dimensions (samples,2)\n",
    "#and make a time t vector of the same length\n",
    "#plot your result and listen over headphones to the result, does it sound lateralized?\n",
    "\n",
    "\n",
    "#plt.figure()\n",
    "#plt.plot(t,sig[:,0])\n",
    "#plt.plot(t,sig[:,1])\n",
    "\n",
    "#sd.play(x, fs)\n",
    "#sd.play(sig, fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Next run the code through two monaural GT filterbanks, and plot the 1 kHz (42) channel\n",
    "gt_out = gammatone_analysis(sig[:,0], fs, numbands, fmin)\n",
    "gt_L = gt_out['bmm']\n",
    "\n",
    "gt_out = gammatone_analysis(sig[:,1], fs, numbands, fmin)\n",
    "gt_R = gt_out['bmm']\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(t,gt_L[42,:])\n",
    "plt.plot(t,gt_R[42,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To mimic the biophysics of auditory processing associated with the cochlear sensory cells (inner-hair-cell) \n",
    "#and auditory nerve synapses, first a half-wave rectification is applied, after which a low-pass filtering is conducted\n",
    "#which functionally models the physiology observation that for carrier frequencies under 1 kHz the signals-phase \n",
    "#information is kept while it is removed for carrier frequencies above 1 kHz (i.e., the auditory phase-locking limit). \n",
    "#index 42 corresponds to the band with a 1-kHz center frequency\n",
    "\n",
    "#%rectification\n",
    "rec_L = gt_L * (gt_L > 0)\n",
    "rec_R = gt_R * (gt_R > 0)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(t,gt_L[42,:])\n",
    "plt.plot(t,rec_L[42,:])\n",
    "plt.title('half-wave rectified [IHC processing]')\n",
    "\n",
    "fCut=1000\n",
    "#LP filtering\n",
    "from scipy.signal import fftconvolve, lfilter\n",
    "beta=np.exp(-fCut/fs);\n",
    "IHC_L=lfilter([1-beta],[1, -beta], rec_L, axis=1)\n",
    "IHC_R=lfilter([1-beta],[1, -beta], rec_R, axis=1)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(t,IHC_L[42,:])\n",
    "plt.plot(t,IHC_R[42,:])\n",
    "plt.title('low-pass filtered [IHC/AN processing]')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make a figure which compares the outputs of gammatone filter with output at 3.5 kHz and compare it to the\n",
    "#gammatone filter with output at 0.2 kHz, do you observe the phase-locking properties? You can zoom in by using np.xlim\n",
    "#To get the model-band index corresponding to a filters center frequency (i.e. characteristic frequency)\n",
    "#you can adopt the following command: Ch1k_gt = np.abs(gt_out['cf'] - CFdes).argmin()    , where \"CFdes [Hz]\" \n",
    "#corresponds to the desired frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This figure shows the frequency spectra before and after applying the filtering, \n",
    "#to validate the success of the filtering\n",
    "\n",
    "plt.figure()\n",
    "ps = 2*abs(np.fft.fft(rec_L[42,:])/len(rec_L[42,:]))**2\n",
    "freq_vect = np.fft.fftfreq(len(rec_L[42,:]), d=1/fs)\n",
    "plt.plot(freq_vect[:int(len(rec_L[42,:])/2)], 10*np.log10(ps[:int(len(rec_L[42,:])/2)]))\n",
    "\n",
    "ps = 2*abs(np.fft.fft(IHC_L[42,:])/len(IHC_L[42,:]))**2\n",
    "freq_vect = np.fft.fftfreq(len(IHC_L[42,:]), d=1/fs)\n",
    "plt.plot(freq_vect[:int(len(IHC_L[42,:])/2)], 10*np.log10(ps[:int(len(IHC_L[42,:])/2)]))\n",
    "plt.legend([\"before filtering\",\"After filtering\"])\n",
    "plt.xlim((0, 2000))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute the interaural cross-correlation at each frequency band\n",
    "maxLag = int(0.001*fs)\n",
    "nCh = gt_L.shape[0]\n",
    "Nx = 10000 #gt_L.shape[1]\n",
    "iaccFuncts = np.zeros((2*maxLag+1, nCh))\n",
    "itdEst = np.zeros(nCh)\n",
    "lags = np.arange(-maxLag, maxLag + 1)\n",
    "lagValues = lags/fs\n",
    "for freqInd in range(nCh):\n",
    "    c = np.correlate(IHC_L[freqInd,:Nx], IHC_R[freqInd,:Nx], mode=2)\n",
    "    #perform the crosscorrelation between the different CF channels\n",
    "    iaccFuncts[:,freqInd] = c[Nx - 1 - maxLag:Nx + maxLag]\n",
    "    # compute the ITD estimate at different frequency bands based on the maxima\n",
    "    # of the IACC functions in a certain time around time = 0, note that c=symmetrical around 0.\n",
    "    itdEst[freqInd] = lagValues[iaccFuncts[:,freqInd].argmax()] # argmax() ==> index of the max "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plotting of the figures\n",
    "plt.figure()\n",
    "plt.semilogx(np.round(gt_out['cf']), itdEst*1000, 'k')\n",
    "plt.ylabel('Estimated ITD [ms]')\n",
    "plt.xlabel('Characteristic Frequency [Hz]')\n",
    "#Around which location do most channels locate the source based on the ITD? Is it the left or right ear? \n",
    "#negative delays mean that R was phase delayed over L."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now make a code which computes the ILD difference between the different channels and ears\n",
    "#You can compute the Level in each channel by computing the rms level in each channel. \n",
    "#Is the ILD estimate consistent with your ITD estimate of the source location?\n",
    "#You can modify the for-loop code from the cell above"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env: project1]",
   "language": "python",
   "name": "project1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
