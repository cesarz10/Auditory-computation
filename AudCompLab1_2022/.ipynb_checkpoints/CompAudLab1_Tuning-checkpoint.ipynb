{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load in necessary modules and packages.\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import scipy.io.wavfile as sio_wav\n",
    "import scipy.signal as sp_sig\n",
    "from filters import (erb_point,erb_space,centre_freqs,make_erb_filters,erb_filterbank,gammatone_analysis,pow_stft,hz2mel,mel2hz,mel_fb,mel_analysis)\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.fftpack import dct\n",
    "from signal_utils import *\n",
    "%matplotlib inline\n",
    "import sounddevice as sd\n",
    "import scipy.io\n",
    "from scipy import signal\n",
    "import keras\n",
    "from keras.models import model_from_json\n",
    "from keras.utils import CustomObjectScope\n",
    "from keras.initializers import glorot_uniform\n",
    "from helper_ops import *\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Main code framework to compare auditory model outputs.\n",
    "#Sarah Verhulst, Deepak Baby, Arthur Van Den Broucke, UGent, 2021\n",
    "\n",
    "# General parameters\n",
    "#framelength_t = 0.025 # framelength in time = 25ms\n",
    "#frameshift_t = 0.01 # frameshift in time = 10ms\n",
    "L = 70. # specify the desired SPL of the input\n",
    "fs = 48e3\n",
    "\n",
    "#Your code to Generate a Pure-tone goes here \n",
    "#Take note that the CoNNear model uses a context window of 256 samples at both the entry\n",
    "#aswell at the end, when making a stimulus, add zero's for 256 samples (20 kHz sampling freq)\n",
    "#to the stimulus to obtain the same output for the GT as the CoNNear model. \n",
    "\n",
    "#output = y\n",
    "\n",
    "#make a 10ms onset/offset Hanning window:\n",
    "#output = x\n",
    "#output = HWin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting part of the stimulus\n",
    "plt.figure(1)\n",
    "plt.plot(t,y)\n",
    "plt.xlabel(\"Time [s]\")\n",
    "plt.ylabel(\"Amplitude\")\n",
    "#plt.xlim((0, 0.02))\n",
    "\n",
    "#plt.figure(2)\n",
    "#plt.plot(t,HWin)\n",
    "#plt.plot(t,x)\n",
    "#plt.xlabel(\"Time [s]\")\n",
    "#plt.ylabel(\"Amplitude\")\n",
    "#plt.xlim((0, 0.02))\n",
    "plt.show()\n",
    "\n",
    "#sd.play(x, fs)\n",
    "#sd.play(y, fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatenate the stimulus with some silence afterwards to see model ringdown\n",
    "#output = x\n",
    "#plt.figure(3)\n",
    "#plt.plot(t,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = adjust_spl(x, L)\n",
    "# parameters for gammatone analysis\n",
    "fmin = 50. #lowest frequency simulated for GT\n",
    "numbands = 64 #number of GT bands\n",
    "fs_nn = 20e3 # CoNNear requires 20kHz\n",
    "\n",
    "# Parameters for the cochleagram figures\n",
    "framelength_t = 0.025/4 # framelength in time \n",
    "frameshift_t = 0.01/4 # frameshift in time \n",
    "#for the GT\n",
    "framelength_gt = int(framelength_t * fs)\n",
    "frameshift_gt = int(frameshift_t * fs)\n",
    "\n",
    "#for the CoNNear model\n",
    "framelength_nn = int(framelength_t * fs_nn)\n",
    "frameshift_nn = int(frameshift_t * fs_nn)\n",
    "\n",
    "# Calculate the gammatone model and cochleagrams\n",
    "gt_out = gammatone_analysis(x, fs, numbands, fmin)\n",
    "gt_cochleagram = cochleagram(gt_out['bmm'], framelength_gt, frameshift_gt)\n",
    "t_gt = np.arange(len(x)) / fs\n",
    "\n",
    "# Calculate the CoNNear model and cochleagrams\n",
    "print(\"Resampling signal to \" + str(fs_nn) + \" Hz\")\n",
    "x_nn = sp_sig.resample_poly(x, fs_nn, fs)\n",
    "x_nn = np.expand_dims(x_nn, axis=0)\n",
    "x_nn = np.expand_dims(x_nn, axis=2)\n",
    "nn_out = connear.predict(x_nn)\n",
    "nn_out = nn_out[0,:,:]\n",
    "nn_cochleagram = cochleagram(nn_out.T, int(framelength_t * fs_nn), int(frameshift_t * fs_nn))\n",
    "t_nn = np.arange(len(x_nn)) / fs_nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The indexes belonging to the 1 kHz frequency channels\n",
    "#You can modify some parts of this code to see the response to a different CF channel \n",
    "Nch,N = np.shape(gt_out['bmm'])\n",
    "Nch_nn,N_nn= np.shape(nn_out)\n",
    "Bch_gt = np.nonzero(gt_out['cf']<1000) #get the CF channels with values < 1000 Hz\n",
    "Bch_gt\n",
    "Ch1k_gt = int(42)\n",
    "Ch1k_nn = int(78) # CF channels with values < 1000 Hz for CoNNear\n",
    "\n",
    "gt = np.zeros((Nch, N)) #copy the data so we can work with it\n",
    "gt = gt_out['bmm'];\n",
    "\n",
    "nn = np.zeros((Nch_nn, N_nn)) #copy the data so we can work with it\n",
    "nn = nn_out.T;\n",
    "\n",
    "#get the 1kHz-CF time-domain signal vs energy signal in specific bins\n",
    "gt_1k = gt[Ch1k_gt,:] #time domain signal gt\n",
    "nn_1k = nn[Ch1k_nn,:] #gt energy in 1-kHz channel, when used as preprocessing for Machine Hearing\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(t_gt,gt_1k/max(gt_1k))\n",
    "plt.plot(t_nn,nn_1k/max(nn_1k),'--')\n",
    "plt.legend(['GT','CoNNear'])\n",
    "#plt.xlim((0.1, 0.12))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate a cochlear excitation pattern\n",
    "#your code goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate a click stimulus C_gt and C_nn: your code goes here\n",
    "#outputs C_gt and C_nn as well as the time vectors: t_nn and t_gt\n",
    "#plt.figure()\n",
    "#plt.plot(t_gt,C_gt)\n",
    "#plt.plot(t_nn,C_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run the stimulus for different levels through the model\n",
    "L_list=np.arange(10, 100, 10)\n",
    "Cs_gt=np.zeros((Nch, N_gt, len(L_list)))\n",
    "Cs_nn=np.zeros((Nch_nn, len(C_nn), len(L_list)))\n",
    "Ps =np.zeros((N_gt, len(L_list)))\n",
    "\n",
    "nCh_1k = 42 #to write out the 1-kHz channel, and example is only made for the GT model here\n",
    "#you should add corresponding code for the CoNNear model\n",
    "for nL,L in enumerate(L_list):\n",
    "    x = adjust_spl(C_gt, L)\n",
    "    result = gammatone_analysis(x, fs, numbands, fmin)\n",
    "    Cs_gt[:, :, nL] = result['bmm']\n",
    "    Ps[:, nL] = (2*abs(np.fft.fft(result['bmm'][nCh_1k, :]))/result['bmm'].shape[1])**2\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(t_gt, Cs_gt[nCh_1k, :, :])\n",
    "\n",
    "# Frequency domain\n",
    "plt.figure()\n",
    "freq_vect = np.fft.fftfreq(N_gt, d=1/fs)\n",
    "plt.plot(freq_vect[:int(N_gt/2)], 10*np.log10(Ps[:int(N_gt/2)]))\n",
    "plt.xlim((0, 8000))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
