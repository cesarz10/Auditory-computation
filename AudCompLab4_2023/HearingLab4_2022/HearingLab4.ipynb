{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import scipy\n",
    "import math\n",
    "import scipy.io.wavfile as wav\n",
    "from scipy import signal\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from helper import Oct3smooth\n",
    "from helper import SII\n",
    "from helper import audioread\n",
    "from helper import rms\n",
    "from helper import SpeechArray\n",
    "from helper import audiowrite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter here the path where the Matrix sentences are stored:\n",
    "dir_where_AFC = \n",
    "# dir_where_AFC = 'H:\\\\Desktop\\\\HearingLab4\\\\AFC_for_lectures_20200427'\n",
    "filesep = os.path.sep\n",
    "dir_where    = dir_where_AFC+filesep+'sounds'+filesep+'vlmatrix'+filesep+'00-original'+filesep # broadband VlMatrix material\n",
    "dir_where_LP = dir_where_AFC+filesep+'sounds'+filesep+'vlmatrix'+filesep+'01-LP'+filesep # here you will store the LP files\n",
    "dir_where_HP = dir_where_AFC+filesep+'sounds'+filesep+'vlmatrix'+filesep+'02-HP'+filesep # here you will store the HP files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_i  = 160*np.power(2, (1./3.)*np.arange(0,18)) # 18 frequencies spaced from 160 Hz and upwards in 1/3 OB\n",
    "freq_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Processing one sound, the speech-shaped noise:\n",
    "# 1. Loading the sound with the custo function audioread \n",
    "fname_noise = dir_where+'VlMatrixnoise_ltass.wav'\n",
    "fs, noise = audioread(fname_noise)\n",
    "\n",
    "# 2. Plotting the spectrum:\n",
    "#    2.1 Fast Fourier Transform\n",
    "N = 2**15\n",
    "K = round(N/2)\n",
    "noise_fft = np.fft.fft(noise,n=N)\n",
    "noise_fft = noise_fft[0:K-1] # only first half of the FFT (the other half is mirrored)\n",
    "f = np.arange(0,K-1)\n",
    "f = f/K*(fs/2) # if you perform N-point FFTs you can use this f for all your plots\n",
    "\n",
    "#    2.2 Converting to dB (note that the amplitudes are relative...):\n",
    "# Spectrum:\n",
    "noise_dB        = 20*np.log10(abs(noise_fft))\n",
    "# Smoothed spectrum:\n",
    "noise_dB_smooth = Oct3smooth(f,noise_dB,freq_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.semilogx(f,noise_dB)\n",
    "\n",
    "plt.plot(freq_i,noise_dB_smooth,'r')\n",
    "plt.xlabel('Frequency [Hz]')\n",
    "plt.ylabel('Relative amplitude [dB]')\n",
    "plt.xlim((50,20000))\n",
    "\n",
    "plt.legend(['FFT','Smoothed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [6]: Include your code here to add the spectrum of one sentence from the materials. \n",
    "#           It may be helpful to add in the same plot the noise spectrum. Please choose a\n",
    "#           clear way to visualise the sentence and noise spectra.\n",
    "\n",
    "# Some help: 'insig1 = SpeechArray(dir_where,1)' will give you one sentence at random, alternatively \n",
    "#                you can also load insig1 as done in Cell[5] with 'noise' using audioread\n",
    "#            'insig10= SpeechArray(dir_where,10)' will give you 10 sentences at random \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [7]: Include your code here to add the spectrum of 20 randomly chosen sentences. \n",
    "#           This code will be similar to that of cell [6] but you have to put first \n",
    "#           the 20 sentences all together as one big sentence (use the SpeechArray function\n",
    "#           from helper.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [8]: Include your code here to assess the speech intelligibility index for LPF signals \n",
    "f2test = [] # Hz, define here the 20 cut-off frequencies you want to test...\n",
    "\n",
    "fc_low = [] # Hz, write down here (or later in the code), the cut-off frequency that gives an SII_value of 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [9]: Include your code here to assess the speech intelligibility index for HPF signals \n",
    "f2test = [] # Hz, define here the 20 cut-off frequencies you want to test...\n",
    "\n",
    "fc_high = [] # Hz, write down here (or later in the code), the cut-off frequency that gives an SII_value of 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [10]\n",
    "filt_order = 4 # order of the Butterworth filter, use the same order for both LPF and HPF speech\n",
    "fc_low_norm = .99 # put here the cut-off frequency normalised as required for signal.butter (use fc_low, of course)\n",
    "\n",
    "lijsten = os.listdir(dir_where)\n",
    "\n",
    "dir_here = dir_where_LP\n",
    "for l in lijsten:\n",
    "    if 'noise' in l: # Skip noise file\n",
    "        print('Noise file skipped {}'.format(l))\n",
    "        continue\n",
    "    \n",
    "    # Load here each insig to process\n",
    "    \n",
    "    fname = dir_where+l # loads the file names with no processing\n",
    "    b, a = signal.butter(filt_order, fc_low_norm, 'low')\n",
    "    # Filter here each insig using b and a coefficients\n",
    "        \n",
    "    # Save the resulting waveforms to file in the directory 'dir_where_LP', use 'audiowrite'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [11]: Similar to Cell[10] but for high-pass filter\n",
    "fc_high_norm = .01 # put here the cut-off frequency normalised as required for signal.butter (use fc_high, of course)\n",
    "\n",
    "dir_here = dir_where_HP\n",
    "for l in lijsten:\n",
    "    if 'noise' in l: # Skip noise file\n",
    "        print('Noise file skipped {}'.format(l))\n",
    "        continue\n",
    "    \n",
    "    # Load here each insig to process\n",
    "    \n",
    "    fname = dir_where+l # loads the file names with no processing\n",
    "    b, a = signal.butter(filt_order, fc_high_norm, 'high')\n",
    "    # Filter here each insig using b and a coefficients\n",
    "        \n",
    "    # Save the resulting waveforms to file in the directory 'dir_where_HP', use 'audiowrite'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If everything went as expected your low-pass filtered and high-pass filtered speech should be in the folders\n",
    "#     01-LP/\n",
    "#     02-HP/\n",
    "#     00-original/ (broadband files, already available)\n",
    "\n",
    "# Don't forget to listen to your sounds to know how LP and HP speech sounds like\n",
    "# Now you should be ready to start the AFC experiment, go to MATLAB!\n",
    "\n",
    "# Success!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
